# ForensIQ â€” PageIndex RAG for UFDR/CLBE Forensics

A **PageIndex RAG** system that ingests Cellebrite **UFDR** and **CLBE** forensic extraction archives, indexes their contents into token-bounded **pages**, and feeds them into two downstream RAGs:

| RAG Layer | Purpose | Backend |
|-----------|---------|---------|
| **Vector RAG** | Page-based semantic embedding & similarity search | FAISS + OpenAI `text-embedding-3-small` |
| **Graph RAG** | Entity extraction & relationship mapping | Neo4j |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ForensIQ Pipeline                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  UFDR    â”‚â”€â”€â”€â–¶â”‚  PageIndex â”‚â”€â”€â”€â–¶â”‚ Vector RAGâ”‚   â”‚ Graph RAGâ”‚â”‚
â”‚  â”‚  Parser  â”‚    â”‚  Indexer   â”‚    â”‚  (FAISS)  â”‚   â”‚  (Neo4j) â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚       â”‚                â”‚                 â”‚               â”‚      â”‚
â”‚  .ufdr / dir     Page objects      Embeddings      Entities +  â”‚
â”‚                  (token-bounded)   + similarity     relationshipsâ”‚
â”‚                                    search                       â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    FastAPI  /api/v1                          â”‚â”‚
â”‚  â”‚  POST /ingest/upload   POST /query   GET /pages/{id}        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
ForensIQ/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py              # Centralised config (pydantic-settings)
â”œâ”€â”€ forensiq/
â”‚   â”œâ”€â”€ ufdr/
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic models for forensic artefacts
â”‚   â”‚   â””â”€â”€ parser.py            # UFDR archive extractor + XML parser
â”‚   â”œâ”€â”€ pageindex/
â”‚   â”‚   â”œâ”€â”€ page.py              # Page model (atomic unit of content)
â”‚   â”‚   â”œâ”€â”€ indexer.py           # Converts extraction â†’ pages (token-bounded)
â”‚   â”‚   â””â”€â”€ store.py             # JSONL-based persistent page store
â”‚   â”œâ”€â”€ vectorrag/
â”‚   â”‚   â”œâ”€â”€ embedder.py          # OpenAI embedding client
â”‚   â”‚   â”œâ”€â”€ faiss_store.py       # FAISS index management
â”‚   â”‚   â””â”€â”€ retriever.py         # Semantic search over pages
â”‚   â”œâ”€â”€ graphrag/
â”‚   â”‚   â”œâ”€â”€ schema.py            # Neo4j node labels & relationship types
â”‚   â”‚   â”œâ”€â”€ neo4j_client.py      # Neo4j driver wrapper
â”‚   â”‚   â””â”€â”€ extractor.py         # Entity extraction + graph population
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â””â”€â”€ pipeline.py          # End-to-end ingest & query orchestrator
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py            # FastAPI endpoints
â”‚   â””â”€â”€ main.py                  # FastAPI app entry-point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_indexer.py
â”‚   â””â”€â”€ test_extractor.py
â”œâ”€â”€ docker-compose.yml           # Neo4j + ForensIQ app
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## Quick Start

### 1. Clone & configure

```bash
cp .env.example .env
# Edit .env â€” set OPENAI_API_KEY and optionally NEO4J_PASSWORD
```

### 2. Install (local dev)

```bash
python -m venv .venv && source .venv/bin/activate
pip install -e ".[dev]"
```

### 3. Start Neo4j

```bash
sudo docker run -d --name forensiq-neo4j \
  -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/forensiq_secret \
  neo4j:5-community
```

### 4. Generate & Run Demo

```bash
# Generate synthetic forensic dataset (Operation Digital Trail)
python tools/generate_demo_dataset.py

# Run the full pipeline demo â€” parses, indexes, populates Neo4j, runs queries
python tools/demo.py
```

Output: **133 nodes**, **468 relationships** from 2 device extractions (29 pages).

### 5. Run the API server

```bash
uvicorn forensiq.main:app --reload
```

The API docs are at **http://localhost:8000/docs**.

### 5. Or run everything via Docker Compose

```bash
docker compose up --build
```

---

## API Endpoints

| Method | Path | Description |
|--------|------|-------------|
| `POST` | `/api/v1/ingest/upload` | Upload a `.ufdr` file for full ingestion |
| `POST` | `/api/v1/ingest/path` | Ingest a UFDR source from a local filesystem path |
| `POST` | `/api/v1/query` | Hybrid semantic + graph search |
| `GET`  | `/api/v1/pages/{extraction_id}` | List all pages for an extraction |
| `GET`  | `/api/v1/stats` | System statistics (extractions, vectors, graph) |
| `GET`  | `/health` | Health check |

### Example: Ingest

```bash
curl -X POST http://localhost:8000/api/v1/ingest/upload \
  -F "file=@/path/to/case.ufdr"
```

### Example: Query

```bash
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Who did the suspect message on WhatsApp about the meeting?", "k": 5}'
```

---

## How It Works

### PageIndex (the core)

1. **UFDR Parser** reads the Cellebrite XML report inside the archive and extracts structured artefacts (contacts, calls, messages, locations, etc.).
2. **Indexer** serialises each artefact category into human-readable text and groups them into **Pages** â€” token-bounded chunks (default 512 tokens) that fit a single embedding call.
3. Pages are persisted as JSONL files.

### Vector RAG

- Each page is embedded via OpenAI `text-embedding-3-small`.
- Embeddings are stored in a **FAISS** inner-product index (cosine similarity via L2-normalised vectors).
- At query time the user's question is embedded and the top-K nearest pages are returned.

### Graph RAG

- **Entity extractor** runs regex + heuristic rules over each page to pull out **Person**, **PhoneNumber**, **EmailAddress**, **App**, **Location**, **URL**, **Organization**, **Account** entities.
- Entities and relationships (CALLED, MESSAGED, HAS_PHONE, BELONGS_TO_ORG, â€¦) are merged into **Neo4j**.
- At query time, entities from the top vector hits are expanded through the graph to find related people, devices, and communication patterns.

---

## Demo: Operation Digital Trail

A synthetic financial fraud investigation with 2 device extractions:

| Character | Role | Device |
|-----------|------|--------|
| Vikram Mehta | Primary suspect, fake import/export firm | OnePlus 12 (121 artifacts) |
| Priya Sharma | Accomplice, Hawala broker | iPhone 15 Pro (38 artifacts) |
| Rajan Patel | Crypto launderer | â€” |
| Deepak Joshi | Bank insider | â€” |
| Ananya Singh | Lawyer, shell company setup | â€” |
| Farid Hassan | Dubai trade partner | â€” |
| Li Wei | Chinese supplier, falsified docs | â€” |

**What the graph reveals:**
- ðŸ”— Cross-device entity correlation (shared phone numbers/emails merge automatically)
- ðŸ’° Financial trail (Hawala, crypto mixers, wire transfers under â‚¹10L)
- ðŸŒ International laundering network (India â†’ Dubai â†’ Mauritius â†’ BVI â†’ China)
- ðŸ”’ Privacy tool usage (Tor, VPN, encrypted messaging, calculator vault)
- ðŸ“ Location timeline linking suspects to warehouses, banks, and meeting points

**Neo4j Browser:** http://localhost:7474 (`neo4j` / `forensiq_secret`)

```cypher
-- Full graph
MATCH (n)-[r]->(m) RETURN n,r,m LIMIT 200

-- Suspect's communication network
MATCH (p:Person)-[r:MESSAGED]-(other:Person)
WHERE p.name CONTAINS 'Vikram'
RETURN p, r, other

-- Organisation network
MATCH (o:Organization)<-[:BELONGS_TO_ORG]-(p:Person)
RETURN o, p
```

---

## Google Drive Integration

ForensIQ can download `.clbe` files directly from Google Drive:

```bash
# Authenticate via OAuth2
curl http://localhost:8000/api/v1/gdrive/auth

# List .clbe files in a Drive folder
curl http://localhost:8000/api/v1/gdrive/list/{folder_id}

# Ingest all .clbe files from a Drive folder
curl -X POST http://localhost:8000/api/v1/gdrive/ingest/{folder_id}
```

---

## Running Tests

```bash
pytest tests/ -v
```

All 10 tests pass.

---

## License

MIT
