"""LLM integration — Gemini (primary) + OpenRouter gpt-oss-120b (cached reframes).

Flow
----
1. **Fresh query** (cache miss):
   RAG context → Gemini 2.0 Flash → polished forensic answer → cached in Redis.

2. **Repeated query** (cache hit):
   Cached answer + new prompt → OpenRouter gpt-oss-120b (free) → reframed answer.
   This avoids burning Gemini quota on repeated questions.
"""

from __future__ import annotations

import json
import logging
from typing import Any

import httpx

logger = logging.getLogger(__name__)

# ── System prompts ────────────────────────────────────

_FORENSIQ_SYSTEM = """\
You are ForensIQ, an expert digital forensics analyst assistant.
You have access to data extracted from mobile device forensic images
(Cellebrite UFDR/CLBE extractions).

STRICT RULES — follow these without exception:
1. ONLY use information that is explicitly present in the "Retrieved Forensic Evidence" section.
2. NEVER invent, assume, or hallucinate any data, names, phone numbers,
   messages, timestamps, or relationships that are not in the evidence.
3. If the evidence does not contain information to answer the question,
   respond with: "No matching records found in the ingested device data."
4. If the evidence only partially answers the question, answer ONLY the
   parts that are supported by the evidence and clearly state what data
   is missing or not found.
5. Cite specific evidence: quote exact text from the evidence, reference
   artifact types (WhatsApp, SMS, call_log, etc.), and mention page IDs
   when available.
6. Do NOT add speculative analysis or "possible interpretations" unless
   directly supported by the evidence text.
7. Keep your response focused on exactly what was asked — do not add
   unrelated information even if it appears in the evidence.
8. Present findings in a structured, clear format.
"""

_REFRAME_SYSTEM = """\
You are a helpful assistant that reformulates cached forensic analysis
answers for slightly different phrasings of the same question.

You will receive:
1. The original cached answer (previously generated by a senior AI analyst).
2. The user's new prompt.

Your job: adapt the cached answer to match the new prompt's exact wording
and emphasis, without changing the underlying facts. Keep the forensic tone.
If the cached answer doesn't cover what the new prompt asks, note what's missing.
"""


class ForensIQLLM:
    """Dual-LLM client: Gemini for fresh answers, OpenRouter for cached reframes."""

    def __init__(
        self,
        gemini_api_key: str = "",
        openrouter_api_key: str = "",
        gemini_model: str = "gemini-2.0-flash",
        openrouter_model: str = "openai/gpt-oss-120b:free",
    ) -> None:
        self._gemini_key = gemini_api_key
        self._openrouter_key = openrouter_api_key
        self._gemini_model = gemini_model
        self._openrouter_model = openrouter_model

        # Validate Gemini
        self._gemini_client = None
        self._gemini = None
        if self._gemini_key:
            try:
                from google import genai
                self._gemini_client = genai.Client(api_key=self._gemini_key)
                self._gemini = True
                logger.info("Gemini LLM ready (model=%s)", self._gemini_model)
            except Exception as exc:
                logger.error("Gemini init failed: %s", exc)

        if not self._openrouter_key:
            logger.warning("OpenRouter API key not set — cached reframes will fall back to Gemini")

    # ── Primary: Gemini for fresh answers ──────────────

    def generate(self, prompt: str, rag_context: str) -> str:
        """Generate a fresh forensic answer using Gemini.

        Args:
            prompt: User's question.
            rag_context: Retrieved evidence from Vector RAG + Graph RAG.

        Returns:
            LLM-generated answer string.
        """
        if not self._gemini:
            return self._generate_openrouter_fallback(prompt, rag_context)

        full_prompt = (
            f"## User Question\n{prompt}\n\n"
            f"## Retrieved Forensic Evidence\n{rag_context}\n\n"
            f"## Instructions\n"
            f"Answer the user's question using ONLY the evidence provided above.\n"
            f"- If the evidence contains relevant information, cite it exactly "
            f"(quote specific messages, names, phone numbers, timestamps).\n"
            f"- If the evidence does NOT contain information relevant to the question, "
            f"respond with: 'No matching records found in the ingested device data.'\n"
            f"- Do NOT invent or assume any facts not present in the evidence above.\n"
            f"- Stay strictly on-topic — only address what was asked."
        )

        try:
            response = self._gemini_client.models.generate_content(
                model=self._gemini_model,
                contents=full_prompt,
                config={
                    "system_instruction": _FORENSIQ_SYSTEM,
                    "temperature": 0.2,
                    "max_output_tokens": 4096,
                },
            )
            return response.text
        except Exception as exc:
            logger.error("Gemini generation failed: %s", exc)
            return f"[LLM Error] Gemini call failed: {exc}"

    # ── Secondary: OpenRouter for cached reframes ──────

    def reframe(self, prompt: str, cached_response: str) -> str:
        """Reframe a cached answer for a new prompt using OpenRouter (free tier).

        Falls back to returning the cached answer as-is if OpenRouter fails.
        """
        if not self._openrouter_key:
            # No OpenRouter key → just return the cached answer
            return f"[Cached] {cached_response}"

        messages = [
            {"role": "system", "content": _REFRAME_SYSTEM},
            {"role": "user", "content": (
                f"## New User Prompt\n{prompt}\n\n"
                f"## Cached Answer (from previous analysis)\n{cached_response}\n\n"
                f"Please adapt the cached answer to address the new prompt exactly."
            )},
        ]

        try:
            with httpx.Client(timeout=30) as client:
                resp = client.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self._openrouter_key}",
                        "Content-Type": "application/json",
                        "HTTP-Referer": "https://forensiq.app",
                        "X-Title": "ForensIQ",
                    },
                    json={
                        "model": self._openrouter_model,
                        "messages": messages,
                        "max_tokens": 2048,
                        "temperature": 0.3,
                    },
                )
                resp.raise_for_status()
                data = resp.json()
                answer = data["choices"][0]["message"]["content"]
                logger.info("OpenRouter reframe OK (model=%s)", self._openrouter_model)
                return answer
        except Exception as exc:
            logger.warning("OpenRouter reframe failed (%s), returning cached answer", exc)
            return f"[Cached] {cached_response}"

    # ── Fallback: OpenRouter as primary when no Gemini ─

    def _generate_openrouter_fallback(self, prompt: str, rag_context: str) -> str:
        """Use OpenRouter as primary LLM if Gemini is unavailable."""
        if not self._openrouter_key:
            return (
                f"[No LLM configured] RAG context retrieved but no Gemini/OpenRouter API key.\n\n"
                f"Raw evidence:\n{rag_context[:2000]}"
            )

        messages = [
            {"role": "system", "content": _FORENSIQ_SYSTEM},
            {"role": "user", "content": (
                f"## User Question\n{prompt}\n\n"
                f"## Retrieved Forensic Evidence\n{rag_context}\n\n"
                f"Analyse the evidence to answer the question."
            )},
        ]

        try:
            with httpx.Client(timeout=60) as client:
                resp = client.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self._openrouter_key}",
                        "Content-Type": "application/json",
                        "HTTP-Referer": "https://forensiq.app",
                        "X-Title": "ForensIQ",
                    },
                    json={
                        "model": self._openrouter_model,
                        "messages": messages,
                        "max_tokens": 4096,
                        "temperature": 0.2,
                    },
                )
                resp.raise_for_status()
                data = resp.json()
                return data["choices"][0]["message"]["content"]
        except Exception as exc:
            logger.error("OpenRouter fallback failed: %s", exc)
            return f"[LLM Error] All LLM calls failed. Raw evidence:\n{rag_context[:2000]}"

    # ── Utility ────────────────────────────────────────

    def status(self) -> dict[str, Any]:
        """Return which LLM backends are available."""
        return {
            "gemini": {
                "available": self._gemini is not None,
                "model": self._gemini_model,
            },
            "openrouter": {
                "available": bool(self._openrouter_key),
                "model": self._openrouter_model,
            },
        }
